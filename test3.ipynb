{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d5293a",
   "metadata": {},
   "source": [
    "## 점수 : 10.13465  등수 :367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d275b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost==1.7.6\n",
      "  Downloading xgboost-1.7.6-py3-none-win_amd64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from xgboost==1.7.6) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from xgboost==1.7.6) (1.15.3)\n",
      "Downloading xgboost-1.7.6-py3-none-win_amd64.whl (70.9 MB)\n",
      "   ---------------------------------------- 0.0/70.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.6/70.9 MB 14.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 7.1/70.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 13.4/70.9 MB 21.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 20.2/70.9 MB 25.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 27.8/70.9 MB 27.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 33.0/70.9 MB 27.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 38.5/70.9 MB 26.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 44.6/70.9 MB 26.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 51.4/70.9 MB 27.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 57.4/70.9 MB 27.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 64.0/70.9 MB 28.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  70.8/70.9 MB 28.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  70.8/70.9 MB 28.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 70.9/70.9 MB 25.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 3.0.3\n",
      "    Uninstalling xgboost-3.0.3:\n",
      "      Successfully uninstalled xgboost-3.0.3\n",
      "Successfully installed xgboost-1.7.6\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mynote\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.7 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 17.3 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed scikit-learn-1.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U xgboost==1.7.6\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"모든 난수 시드 고정\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# ======================\n",
    "# 1) 데이터 불러오기\n",
    "# ======================\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI goorm project1/dataset/train.csv', encoding='utf-8')\n",
    "test_df  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI goorm project1/dataset/test.csv',  encoding='utf-8')\n",
    "building_info = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI goorm project1/dataset/building_info.csv', encoding='utf-8')\n",
    "\n",
    "# ======================\n",
    "# 2) 빌딩 메타 전처리 (강화)\n",
    "# ======================\n",
    "# '-' → NaN → 0 처리(먼저 NaN으로 바꾼 뒤, 숫자형 변환이 쉬움)\n",
    "building_info = building_info.replace('-', np.nan)\n",
    "\n",
    "# 숫자형 컬럼들 명시적 변환 (데이터셋 컬럼명에 맞추세요)\n",
    "num_cols = ['연면적(m2)', '냉방면적(m2)', '태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']\n",
    "for c in num_cols:\n",
    "    if c in building_info.columns:\n",
    "        building_info[c] = pd.to_numeric(building_info[c], errors='coerce')\n",
    "\n",
    "# 결측치 0 채움\n",
    "building_info[num_cols] = building_info[num_cols].fillna(0)\n",
    "\n",
    "# 건물유형 원-핫\n",
    "if '건물유형' in building_info.columns:\n",
    "    building_type_dummies = pd.get_dummies(building_info['건물유형'], prefix='건물유형')\n",
    "    building_info = pd.concat([building_info.drop(columns=['건물유형']), building_type_dummies], axis=1)\n",
    "\n",
    "# 면적/설비 비율 파생\n",
    "if set(['연면적(m2)','냉방면적(m2)']).issubset(building_info.columns):\n",
    "    building_info['냉방면적비'] = (building_info['냉방면적(m2)'] / (building_info['연면적(m2)'] + 1e-6)).clip(0, 1)\n",
    "\n",
    "for col, newname in [\n",
    "    ('태양광용량(kW)',  '태양광_면적당용량'),\n",
    "    ('ESS저장용량(kWh)', 'ESS_면적당용량'),\n",
    "    ('PCS용량(kW)',    'PCS_면적당용량'),\n",
    "]:\n",
    "    if col in building_info.columns and '연면적(m2)' in building_info.columns:\n",
    "        building_info[newname] = building_info[col] / (building_info['연면적(m2)'] + 1e-6)\n",
    "\n",
    "# ======================\n",
    "# 3) 병합\n",
    "# ======================\n",
    "train_df = pd.merge(train_df, building_info, on='건물번호', how='left')\n",
    "test_df  = pd.merge(test_df,  building_info, on='건물번호', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4) 시간·날씨 파생\n",
    "# ======================\n",
    "train_df['일시'] = pd.to_datetime(train_df['일시'], format='%Y%m%d %H')\n",
    "test_df['일시']  = pd.to_datetime(test_df['일시'],  format='%Y%m%d %H')\n",
    "\n",
    "def add_time_weather_features(df):\n",
    "    df['month'] = df['일시'].dt.month\n",
    "    df['day'] = df['일시'].dt.day\n",
    "    df['hour'] = df['일시'].dt.hour\n",
    "    df['dayofweek'] = df['일시'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "\n",
    "    # 주기형 인코딩\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow_sin']  = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dow_cos']  = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "    # 체감 온도 THI, 냉/난방 지표\n",
    "    if set(['기온(°C)','습도(%)']).issubset(df.columns):\n",
    "        df['THI'] = df['기온(°C)'] - 0.55*(1 - (df['습도(%)']/100.0))*(df['기온(°C)'] - 14.5)\n",
    "        df['CDH'] = np.maximum(0, df['THI'] - 24)  # Cooling Degree Hours(임의 임계: 24)\n",
    "        df['HDH'] = np.maximum(0, 18 - df['THI'])  # Heating Degree Hours(임의 임계: 18)\n",
    "    else:\n",
    "        df['THI'] = 0.0\n",
    "        df['CDH'] = 0.0\n",
    "        df['HDH'] = 0.0\n",
    "\n",
    "    # 업무시간 플래그(대략 9~18시)\n",
    "    df['is_business_hour'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = add_time_weather_features(train_df)\n",
    "test_df  = add_time_weather_features(test_df)\n",
    "\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 5) 결측치 처리(안전 버전)\n",
    "# ======================\n",
    "\n",
    "# (선택) 수치형처럼 보이는 날씨 컬럼을 숫자로 강제 변환\n",
    "num_like = ['일조(hr)', '일사(MJ/m2)', '기온(°C)', '습도(%)']\n",
    "for df in [train_df, test_df]:\n",
    "    for c in num_like:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# 수치형 컬럼 집합(둘 중 하나에라도 있으면 후보로)\n",
    "num_cols_train = set(train_df.select_dtypes(include=[np.number]).columns)\n",
    "num_cols_test  = set(test_df.select_dtypes(include=[np.number]).columns)\n",
    "num_cols_union = sorted((num_cols_train | num_cols_test) - {'전력소비량(kWh)'})\n",
    "\n",
    "for c in num_cols_union:\n",
    "    # --- train 쪽 채움 ---\n",
    "    if c in train_df.columns:\n",
    "        if train_df[c].isna().any():\n",
    "            train_df[c] = train_df.groupby('건물번호')[c].transform(lambda s: s.fillna(s.median()))\n",
    "            train_df[c] = train_df[c].fillna(train_df[c].median())\n",
    "\n",
    "    # --- test 쪽 채움 ---\n",
    "    if c in test_df.columns:\n",
    "        if test_df[c].isna().any():\n",
    "            # 1) 건물번호 그룹 중앙값 -> 2) train 전역 중앙값 -> 3) test 전역 중앙값\n",
    "            test_df[c] = test_df.groupby('건물번호')[c].transform(lambda s: s.fillna(s.median()))\n",
    "            if c in train_df.columns:\n",
    "                test_df[c] = test_df[c].fillna(train_df[c].median())\n",
    "            test_df[c] = test_df[c].fillna(test_df[c].median())\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 6) OOF 타깃 인코딩(누설 방지)\n",
    "#    - 건물, 건물×월, 건물×시간 평균 전력\n",
    "# ======================\n",
    "def add_oof_target_mean(train, test, key_cols, target_col='전력소비량(kWh)', n_splits=5):\n",
    "    name = 'TE_' + '_'.join(key_cols)\n",
    "    train = train.sort_values('일시').copy()\n",
    "    tr_values = np.zeros(len(train), dtype=float)\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for tr_idx, va_idx in tss.split(train):\n",
    "        tr_part = train.iloc[tr_idx]\n",
    "        va_part = train.iloc[va_idx]\n",
    "        mapping = tr_part.groupby(key_cols)[target_col].mean()\n",
    "        tr_values[va_idx] = va_part[key_cols].merge(\n",
    "            mapping.rename(name), left_on=key_cols, right_index=True, how='left'\n",
    "        )[name].values\n",
    "    global_mean = train[target_col].mean()\n",
    "    train[name] = pd.Series(tr_values).fillna(global_mean)\n",
    "\n",
    "    # test는 전체 train으로 학습한 mapping 사용\n",
    "    mapping_full = train.groupby(key_cols)[target_col].mean()\n",
    "    test[name] = test[key_cols].merge(\n",
    "        mapping_full.rename(name), left_on=key_cols, right_index=True, how='left'\n",
    "    )[name].fillna(global_mean).values\n",
    "    return train, test\n",
    "\n",
    "for keys in [['건물번호'], ['건물번호','month'], ['건물번호','hour']]:\n",
    "    train_df, test_df = add_oof_target_mean(train_df, test_df, keys)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 7) 학습/검증 분할 (최근 N일 hold-out)\n",
    "# ======================\n",
    "val_days = 14  # 최근 14일을 검증으로\n",
    "cutoff = train_df['일시'].max() - pd.Timedelta(days=val_days)\n",
    "train_part = train_df[train_df['일시'] <= cutoff].copy()\n",
    "valid_part = train_df[train_df['일시'] >  cutoff].copy()\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 8) 피처 선택/정렬  (그대로 OK)\n",
    "# ======================\n",
    "def safe_drop(df, cols):\n",
    "    return df.drop(columns=[c for c in cols if c in df.columns], errors='ignore')\n",
    "\n",
    "drop_common = ['num_date_time', '일시', '일조(hr)', '일사(MJ/m2)']\n",
    "\n",
    "train_y = train_df['전력소비량(kWh)']\n",
    "train_x = safe_drop(train_df, drop_common + ['전력소비량(kWh)'])\n",
    "test_x  = safe_drop(test_df,  drop_common)\n",
    "\n",
    "# test에 없는 컬럼은 0으로 채워 맞추기\n",
    "missing_in_test = [c for c in train_x.columns if c not in test_x.columns]\n",
    "for c in missing_in_test:\n",
    "    test_x[c] = 0\n",
    "test_x = test_x[train_x.columns]\n",
    "\n",
    "# ======================\n",
    "# 8.5) 시간 기반 검증 분할 (최근 14일을 검증으로)\n",
    "# ======================\n",
    "cutoff = train_df['일시'].max() - pd.Timedelta(days=14)\n",
    "tr_mask = train_df['일시'] <= cutoff\n",
    "va_mask = ~tr_mask\n",
    "\n",
    "X_tr = train_x.loc[tr_mask].copy()\n",
    "y_tr = np.log1p(train_y.loc[tr_mask])   # 로그 변환\n",
    "X_va = train_x.loc[va_mask].copy()\n",
    "y_va = np.log1p(train_y.loc[va_mask])\n",
    "\n",
    "# test도 학습 컬럼 순서와 동일하게\n",
    "X_te = test_x.reindex(columns=X_tr.columns, fill_value=0)\n",
    "\n",
    "# ======================\n",
    "# 9) 모델 학습 (조기 종료 + 합리적 파라미터)\n",
    "# ======================\n",
    "model = XGBRegressor(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=8,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',   \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_va, y_va)],\n",
    "    early_stopping_rounds=200,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 10) 검증 성능 & 예측\n",
    "# ======================\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# 기존 코드 교체\n",
    "va_pred_log = model.predict(X_va)\n",
    "va_rmse = rmse(y_va, va_pred_log)\n",
    "print(f\"[Val LOG RMSE] {va_rmse:.5f}\")\n",
    "\n",
    "va_pred = np.expm1(va_pred_log)\n",
    "val_rmse_real = rmse(np.expm1(y_va), va_pred)\n",
    "print(f\"[Val RMSE(real)] {val_rmse_real:.3f}\")\n",
    "\n",
    "# ======================\n",
    "# 11) 테스트 예측\n",
    "# ======================\n",
    "te_pred_log = model.predict(X_te)\n",
    "te_pred = np.expm1(te_pred_log)\n",
    "\n",
    "# ======================\n",
    "# 12) 제출 파일 저장 + 중요도\n",
    "# ======================\n",
    "submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI goorm project1/dataset/sample_submission.csv')\n",
    "submission['answer'] = te_pred\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"모델 학습/검증/예측 완료. submission.csv 생성!\")\n",
    "\n",
    "imp = pd.Series(model.feature_importances_, index=X_tr.columns).sort_values(ascending=False)\n",
    "print(\"\\n[Top 20 Feature Importances]\")\n",
    "print(imp.head(20))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
